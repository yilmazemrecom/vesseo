import os
import shutil
import re
from typing import Counter
from PIL import Image
from bs4 import BeautifulSoup
from fastapi import APIRouter, UploadFile, File, HTTPException, Form, BackgroundTasks
from fastapi.responses import JSONResponse
import unicodedata
import time
from fastapi import Depends
from app.auth import get_current_user
from app.basic import usage_counter
from fastapi import Request
import json
import uuid
from app.config import UPLOAD_DIR
from app.config import get_db
from datetime import datetime
from textstat import flesch_reading_ease
from textblob import TextBlob

router = APIRouter()


UPLOAD_DIR = "app/uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)  # ğŸ“Œ EÄŸer yoksa klasÃ¶rÃ¼ oluÅŸtur

ALLOWED_EXTENSIONS = {".jpg", ".jpeg", ".png", ".gif", ".webp"}
STOPWORDS = {
    "ve", "ile", "da", "de", "iÃ§in", "bu", "o", "ÅŸu", "o", "Ã§ok", "gibi", "ancak",
    "fakat", "veya", "ama", "ise", "diÄŸer", "olan", "bir", "bu", "o", "ÅŸu", "biz", "siz"
}


router = APIRouter()

# ğŸ“Œ GÃ¶rsel YÃ¼kleme ve Analiz
@router.post("/upload-image/")
async def upload_image(file: UploadFile = File(...)):
    """CKEditor'den gelen dosyayÄ± alÄ±p sunucuya kaydeder ve analiz eder."""
    try:
        if not file:
            raise HTTPException(status_code=400, detail="Dosya yÃ¼klenemedi!")

        file_ext = os.path.splitext(file.filename)[1].lower()

        if file_ext not in ALLOWED_EXTENSIONS:
            raise HTTPException(status_code=400, detail="Bu dosya formatÄ± desteklenmiyor!")

        # ğŸ“Œ BugÃ¼nÃ¼n tarihini al ve klasÃ¶r ismini oluÅŸtur
        today = datetime.now()
        year = today.strftime("%Y")  # YÄ±l (2025)
        month = today.strftime("%m")  # Ay (03)
        day = today.strftime("%d")  # GÃ¼n (13)

        # ğŸ“Œ YÄ±l, Ay ve GÃ¼n klasÃ¶rlerini oluÅŸtur
        upload_path = os.path.join(UPLOAD_DIR, year, month, day)
        os.makedirs(upload_path, exist_ok=True)  # EÄŸer klasÃ¶r yoksa oluÅŸtur

        # ğŸ“Œ Benzersiz dosya adÄ± oluÅŸtur (UUID)
        unique_filename = f"{uuid.uuid4().hex}{file_ext}"
        file_path = os.path.join(upload_path, unique_filename)

        # ğŸ“Œ DosyayÄ± kaydet
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        return JSONResponse(status_code=200, content={
            "uploaded": 1,
            "fileName": unique_filename,
            "url": f"/uploads/{year}/{month}/{day}/{unique_filename}"  # ğŸ”— DoÄŸru URL'yi dÃ¶ndÃ¼r
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Dosya yÃ¼klenirken hata oluÅŸtu: {str(e)}")


def delete_old_files(upload_dir, max_age_seconds):
    """Belirli bir sÃ¼reden eski olan gÃ¶rselleri otomatik sil."""
    current_time = time.time()

    for filename in os.listdir(upload_dir):
        file_path = os.path.join(upload_dir, filename)
        timestamp_path = file_path + ".timestamp"

        if filename.endswith((".jpg", ".jpeg", ".png", ".gif", ".webp")):
            # ğŸ“Œ Timestamp dosyasÄ± var mÄ±?
            if os.path.exists(timestamp_path):
                with open(timestamp_path, "r") as ts_file:
                    created_time = float(ts_file.read().strip())

                # ğŸ“Œ EÄŸer dosya belirtilen sÃ¼reden eskiyse sil
                if current_time - created_time > max_age_seconds:
                    try:
                        os.remove(file_path)  # GÃ¶rseli sil
                        os.remove(timestamp_path)  # Timestamp dosyasÄ±nÄ± sil
                        print(f"ğŸ—‘ï¸ Silindi: {file_path}")
                    except Exception as e:
                        print(f"âš ï¸ {file_path} silinemedi: {str(e)}")


def check_alt_tags(content):
    """Ä°Ã§eriÄŸin iÃ§inde gÃ¶rsellerin ALT metinlerini kontrol eder."""
    img_tags = re.findall(r'<img [^>]*>', content)
    
    if not img_tags:
        return {
            "status": "no_images",
            "message": "Ä°Ã§erikte hiÃ§ gÃ¶rsel bulunamadÄ±! En az bir gÃ¶rsel eklemelisiniz.",
            "missing_alt_images": []
        }

    missing_alt = []

    for img in img_tags:
        if 'alt="' not in img:
            missing_alt.append(img)

    if missing_alt:
        return {
            "status": "Eksik ALT",
            "message": f"{len(missing_alt)} gÃ¶rselde ALT etiketi eksik!",
            "missing_alt_images": missing_alt
        }
    return {
        "status": "Tamam",
        "message": "TÃ¼m gÃ¶rseller ALT etiketine sahip.",
        "missing_alt_images": []
    }

def analyze_image(image_path):
    """GÃ¶rselin formatÄ±nÄ± ve boyutunu analiz eder."""
    try:
        with Image.open(image_path) as img:
            image_format = img.format.upper()

        file_size_bytes = os.path.getsize(image_path)
        file_size_kb = file_size_bytes / 1024
        file_size_mb = file_size_kb / 1024

        size_status = "Uygun"
        if file_size_kb > 300:
            size_status = f"Ã‡ok bÃ¼yÃ¼k ({file_size_kb:.2f} KB), sÄ±kÄ±ÅŸtÄ±rÄ±lmasÄ± Ã¶nerilir"

        format_status = "Uygun"
        if image_format not in ["WEBP"]:
            format_status = "WebP formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi Ã¶nerilir"

        return {
            "format": image_format,
            "file_size_kb": round(file_size_kb, 2),
            "file_size_mb": round(file_size_mb, 2),
            "size_status": size_status,
            "format_status": format_status
        }

    except Exception as e:
        return {"error": f"GÃ¶rsel analizi yapÄ±lÄ±rken hata oluÅŸtu: {str(e)}"}


def extract_images_from_content(content):
    """CKEditor iÃ§indeki tÃ¼m gÃ¶rsellerin URL'lerini Ã§Ä±karÄ±r."""
    img_urls = re.findall(r'<img[^>]+src="([^">]+)"', content)
    return img_urls


def clean_html(content):
    """HTML etiketlerini temizleyerek dÃ¼z metni Ã§Ä±kar"""
    soup = BeautifulSoup(content, "html.parser")
    return soup.get_text(separator=" ")

def analyze_readability(content):
    """Okunabilirlik Skoru (Flesch-Kincaid) hesapla"""
    text = clean_html(content)
    return flesch_reading_ease(text)

def analyze_keyword_density(content):
    """Anahtar kelime sÄ±klÄ±ÄŸÄ±nÄ± analiz et (Stopwords filtrelenmiÅŸ)"""
    soup = BeautifulSoup(content, "html.parser")
    text = soup.get_text(separator=" ")  # HTML etiketlerinden arÄ±ndÄ±r

    words = re.findall(r'\b[a-zA-ZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ]+\b', text.lower())  # Sadece kelimeleri al
    filtered_words = [word for word in words if word not in STOPWORDS]  # Stopwords Ã§Ä±kar
    
    word_counts = Counter(filtered_words)
    return word_counts.most_common(5)  # En sÄ±k geÃ§en 5 anlamlÄ± kelimeyi gÃ¶ster

def analyze_sentence_length(content):
    """CÃ¼mle uzunluklarÄ±nÄ± analiz et"""
    text = clean_html(content)
    sentences = re.split(r'[.!?]', text)
    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence]
    return sentence_lengths

def analyze_sentiment(content):
    """Ä°Ã§eriÄŸin pozitif/negatif olup olmadÄ±ÄŸÄ±nÄ± analiz et"""
    text = clean_html(content)
    sentiment = TextBlob(text).sentiment.polarity
    if sentiment > 0:
        return "Pozitif"
    elif sentiment < 0:
        return "Negatif"
    else:
        return "TarafsÄ±z"

@router.post("/content-analysis/")
def analyze_content(request: Request, title: str = Form(...), meta_desc: str = Form(...), content: str = Form(...)):
    """Ä°Ã§eriÄŸi analiz eder"""
    user = get_current_user(request)  # âœ… KullanÄ±cÄ±yÄ± burada Ã§ekiyoruz
    if not isinstance(user, str):  # EÄŸer kullanÄ±cÄ± bilgisi Ã§ekilemezse hata dÃ¶n
        raise HTTPException(status_code=401, detail="Kimlik doÄŸrulama baÅŸarÄ±sÄ±z")

    print(f"âœ… KullanÄ±cÄ±: {user}")
    print(f"âœ… Gelen BaÅŸlÄ±k: {title}")
    print(f"âœ… Gelen Meta AÃ§Ä±klamasÄ±: {meta_desc}")
    print(f"âœ… Gelen Ä°Ã§erik: {content}")

    word_count = len(re.findall(r'\w+', content))
    h1_count = content.count("<h1>")
    h2_count = content.count("<h2>")
    h3_count = content.count("<h3>")
    img_urls = extract_images_from_content(content)
    alt_analysis = check_alt_tags(content)
    readability_score = analyze_readability(content)
    keyword_density = analyze_keyword_density(content)
    sentence_lengths = analyze_sentence_length(content)
    sentiment = analyze_sentiment(content)
    recommendations = []
    successes = []

    # ğŸ“Œ Ä°Ã§erikteki tÃ¼m gÃ¶rselleri al
    img_urls = extract_images_from_content(content)
    image_analysis_results = []

    # ğŸ“Œ YÄ±l / Ay / GÃ¼n bazlÄ± dosya yolu oluÅŸtur
    today = datetime.now()
    year = today.strftime("%Y")  # YÄ±l (2025)
    month = today.strftime("%m")  # Ay (03)
    day = today.strftime("%d")  # GÃ¼n (14)

    for img_url in img_urls:
        img_filename = os.path.basename(img_url)  # URL'den dosya adÄ±nÄ± al
        img_path = os.path.join(UPLOAD_DIR, year, month, day, img_filename)  # ğŸ“Œ DoÄŸru klasÃ¶r yapÄ±sÄ±

        # ğŸ”¹ GÃ¶rselin tam URL yolunu oluÅŸtur (Ã¶rnek: /uploads/2025/03/14/image.jpg)
        file_path = f"/uploads/{year}/{month}/{day}/{img_filename}"  

        if os.path.exists(img_path):
            analysis_result = analyze_image(img_path)  # GÃ¶rsel analizi yap
            
            image_analysis_results.append({
                "file_name": img_filename,
                "file_path": file_path,  # ğŸ”¹ TAM PATH EKLENDÄ°
                "analysis": analysis_result
            })
        else:
            image_analysis_results.append({
                "file_name": img_filename,
                "file_path": None,  # ğŸ”¹ EÄŸer gÃ¶rsel bulunamazsa None olarak ekle
                "error": "ğŸš¨ GÃ¶rsel sunucuda bulunamadÄ±!"
            })


    # ğŸ¯ BaÅŸlÄ±k KontrolÃ¼
    if len(title) < 50:
        recommendations.append("BaÅŸlÄ±k Ã§ok kÄ±sa. En az 50 karakter olmalÄ±dÄ±r.")
    elif len(title) > 60:
        recommendations.append("BaÅŸlÄ±k Ã§ok uzun. 60 karakteri aÅŸmamalÄ±dÄ±r.")
    else:
        successes.append("BaÅŸlÄ±k uzunluÄŸu ideal! âœ…")

    # ğŸ¯ Meta AÃ§Ä±klamasÄ± KontrolÃ¼
    if len(meta_desc) < 120:
        recommendations.append("Meta aÃ§Ä±klamasÄ± Ã§ok kÄ±sa. En az 120 karakter olmalÄ±dÄ±r.")
    elif len(meta_desc) > 160:
        recommendations.append("Meta aÃ§Ä±klamasÄ± Ã§ok uzun. 160 karakteri aÅŸmamalÄ±dÄ±r.")
    else:
        successes.append("Meta aÃ§Ä±klamasÄ± ideal uzunlukta! âœ…")

    # ğŸ¯ Ä°Ã§erik Kelime SayÄ±sÄ± KontrolÃ¼
    if word_count < 300:
        recommendations.append("Ä°Ã§erik Ã§ok kÄ±sa. En az 300 kelime olmalÄ±dÄ±r.")
    elif word_count > 2000:
        recommendations.append("Ä°Ã§erik Ã§ok uzun. Daha net ve Ã¶z yazmalÄ±sÄ±nÄ±z.")
    else:
        successes.append("Ä°Ã§erik uzunluÄŸu mÃ¼kemmel! âœ…")

    # ğŸ¯ H2 BaÅŸlÄ±k SayÄ±sÄ± KontrolÃ¼
    if h2_count < 2:
        recommendations.append("En az 2 tane H2 baÅŸlÄ±ÄŸÄ± eklenmelidir.")
    else:
        successes.append("H2 baÅŸlÄ±k sayÄ±sÄ± ideal! âœ…")

    # ğŸ“Œ Analiz sonucunu JSON formatÄ±na Ã§evir
    analysis_result = {
        "word_count": word_count,
        "h1_count": h1_count,
        "h2_count": h2_count,
        "h3_count": h3_count,
        "image_count": len(img_urls),
        "recommendations": recommendations,
        "successes": successes
    }

    # ğŸ“Œ Analizi veritabanÄ±na kaydet
    conn = get_db()
    cursor = conn.cursor()
    cursor.execute(
        """
        INSERT INTO user_analyses (username, title, meta_desc, content, word_count, h1_count, h2_count, h3_count, image_count, analysis_result)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """,
        (user, title, meta_desc, content, word_count, h1_count, h2_count, h3_count, len(img_urls), json.dumps(analysis_result))
    )
    conn.commit()
    conn.close()

    usage_counter(user)  # KullanÄ±cÄ±nÄ±n analiz sayÄ±sÄ±nÄ± artÄ±r

    try:
        response_data = {
            "title_length": len(title),
            "meta_desc_length": len(meta_desc),
            "word_count": word_count,
            "h1_count": h1_count,
            "h2_count": h2_count,
            "h3_count": h3_count,
            "alt_analysis": alt_analysis,
            "recommendations": recommendations,
            "successes": successes,
            "image_analysis": image_analysis_results,
            "image_count" : len(img_urls),
            "readability_score": readability_score,
            "keyword_density": keyword_density,
            "sentence_lengths": sentence_lengths,
            "sentiment": sentiment,
            }

        return JSONResponse(content=response_data, status_code=200)

    except Exception as e:
        print(f"âŒ SEO Analizi sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}")
        return JSONResponse(
            content={
                "status": "error",
                "message": f"SEO Analizi sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
            },
            status_code=500
        )
